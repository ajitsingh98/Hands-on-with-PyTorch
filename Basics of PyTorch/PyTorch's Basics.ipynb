{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style=\"color:green;\">PyTorch 101</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hi,** This notebook is about introduction to Pytorch and It covers PyTorch capabalities and Hands-on with PyTorch basics. I have covered the building block of PyTorch data type i.e Tensors and basics as well as advanced operations that we can perform on them.\n",
    "\n",
    "In the intial sections I have covered basics of pytorch and it is all about to familiarize yourself with overall ecosystem of PyTorch and in later sections I have covered some advanced topics that includes matrix operations, datasets and dataloader and a neural network implementation for regression task.\n",
    "\n",
    "After going through this notebook you will have fair idea about PyTorch's capabilities and its application in real world problems. You will be able to implement this awesome framework in your current setup or can write it from scratch.\n",
    "\n",
    "\n",
    "\n",
    "This notebook is part of my repo `Master PyTorch` which coveres almost everything you need to know about PyTorch. This repo has projects from most of the ML/DL domains like from `Tabular` to `Images` and `Texts` data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "\n",
    "- Introduction\n",
    "- Tensors\n",
    "- Tensor attributes\n",
    "- Basic Operations on Tensors\n",
    "- Advance Operations on Tensors\n",
    "- Dataset and DataLoader\n",
    "- Autograd\n",
    "- Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What is PyTorch?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- PyTorch is a deep learning framework that offers a wide range of tools and libraries for easy and effective neural network implementation.\n",
    "\n",
    " - It has gained significant popularity and is widely used as an open-source framework. \n",
    "\n",
    "- PyTorch caters to various requirements, making it suitable for both straightforward and complex neural network tasks. \n",
    "\n",
    "- Primarily written in Python, it also provides support for multiple programming languages, including Java and C++.\n",
    "\n",
    "-  For more info you can visit its official website <a href=\"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\" target=\"blank\">pytorch</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of PyTorch\n",
    "\n",
    "PyTorch has a wide range of applications across various fields:\n",
    "\n",
    "- Image and Video Processing: PyTorch provides easy implimentations of CNNs which are quite useful for Image and Video related tasks.\n",
    "\n",
    "- Natural Language Processing (NLP): PyTorch is employed in NLP tasks, including sentiment analysis, machine translation, text generation, named entity recognition, and question-answering systems.\n",
    "\n",
    "- Time Series Analysis: PyTorch is applied in time series forecasting, anomaly detection, and sequence modeling tasks.\n",
    "\n",
    "These are just a few examples of the diverse applications of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Tensors?**\n",
    "\n",
    "- Tensors are the fundamental blocks that carry information in our mathematical models, and they are composed using several operations to create mathematical graphs in which information can flow (propagate) forward (functional application) and backwards (using the chain rule). \n",
    "\n",
    "- Tensors are a specialized data structure that are very similar to arrays and matrices. \n",
    "\n",
    "- In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: what is a tensor __really__? \n",
    "\n",
    "- Tensors are important mathematical objects with applications in multiple domains in mathematics and physics. \n",
    "- The term \"tensor\" comes from the usage of these mathematical objects to describe the stretching of a volume of matter under *tension*. \n",
    "- They are central objects of study in a subfield of mathematics known as differential geometry, which deals with the geometry of continuous vector spaces.\n",
    "-  As a very high-level summary (and as first approximation), tensors are defined as multi-linear \"machines\" that have a number of slots (their order, a.k.a. rank), taking a number of \"column\" vectors and \"row\" vectors *to produce a scalar*.\n",
    "- For example, a tensor $\\mathbf{A}$ (represented by a matrix with rows and columns that you could write on a sheet of paper) can be thought of having two slots. So when $\\mathbf{A}$ acts upon a column vector $\\mathbf{v}$ and a row vector $\\mathbf{x}$, it returns a scalar:\n",
    "\n",
    "$$\\mathbf{A}(\\mathbf{x}, \\mathbf{v}) = s$$\n",
    "    \n",
    "If $\\mathbf{A}$ only acts on the column vector, for example, the result will be another column tensor $\\mathbf{u}$ of one order less than the order of $\\mathbf{A}$. Thus, when $\\mathbf{v}$ acts is similar to \"removing\" its slot: \n",
    "\n",
    "$$\\mathbf{u} = \\mathbf{A}(\\mathbf{v})$$\n",
    "\n",
    "The resulting $\\mathbf{u}$ can later interact with another row vector to produce a scalar or be used in any other way. \n",
    "\n",
    "This can be a very powerful way of thinking about tensors, as their slots can guide you when writing code, especially given that PyTorch has a _functional_ approach to modules in which this view is very much highlighted. As we will see below, these simple equations above have a completely straightforward representation in the code. In the end, most of what our models will do is to process the input using this type of functional application so that we end up having a tensor output and a scalar value that measures how good our output is with respect to the real output value in the dataset.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between numpy's ndarray and Tensors**\n",
    "\n",
    "- Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerator.\n",
    "- Tensors are also optimized for automatic differentiation. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: why not `import pytorch`? The name of the package is `torch` for historical reasons: `torch` is the orginal name of the ancestor of the PyTorch library that got started back in 2002 as a C library with Lua scripting. It was only much later that the original `torch` was ported to Python. The PyTorch project decided to prefix the Py to make clear that this library refers to the Python version, as it was confusing back then to know which `torch` one was referring to. All the internal references to the library use just `torch`. It's possible that PyTorch will be renamed at some point, as the original `torch` is no longer maintained and there is no longer confusion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the version installed and determine whether or not we have a GPU-enabled PyTorch install by issuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.0.1\n",
      "GPU-enabled installation? False\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"GPU-enabled installation? {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of a Tensor**\n",
    "\n",
    "Types of tensors:\n",
    "    \n",
    "- A scalar, a single number, is a zero-th order tensor.\n",
    "    \n",
    "- A column vector $v$ of dimensionality $d_c \\times 1$ is a tensor of order 1.\n",
    "    \n",
    "- A row vector $x$ of dimensionality $1 \\times d_r$ is a tensor of order 1.\n",
    "    \n",
    "- A matrix $A$ of dimensionality $d_r \\times d_c$ is a tensor of order 2.\n",
    "    \n",
    "- A cube $T$ of dimensionality $d_r \\times d_c \\times d_d$ is a tensor of order 3. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Tensor\n",
    "\n",
    "Tensor can be initialized into several ways.\n",
    "\n",
    "Let's get started with tensors in PyTorch. The framework supports eight different types ([Lapan 2018](#References)):\n",
    "\n",
    "- 3 float types (16-bit, 32-bit, 64-bit): `torch.FloatTensor` is the class name for the commonly used 32-bit tensor.\n",
    "- 5 integer types (signed 8-bit, unsigned 8-bit, 16-bit, 32-bit, 64-bit): common tensors of these types are the 8-bit unsigned tensor `torch.ByteTensor` and the 64-bit `torch.LongTensor`.\n",
    "\n",
    "There are three fundamental ways to create tensors in PyTorch ([Lapan 2018](#References)):\n",
    "\n",
    "- Call a tensor constructor of a given type, which will create a non-initialized tensor. So we then need to fill this tensor later to be able to use it.\n",
    "- Call a built-in method in the `torch` module that returns a tensor that is already initialized.\n",
    "- Use the PyTorch–NumPy bridge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "arr_t = torch.tensor(arr)\n",
    "arr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also specify the data type\n",
    "arr = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "arr_t = torch.tensor(arr, dtype=torch.float32)\n",
    "arr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternate way Tensor -> Float Tensor\n",
    "arr = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "arr_t = torch.Tensor(arr)\n",
    "arr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "arr_t = torch.LongTensor(arr)\n",
    "arr_t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1,2,3,4,5])\n",
    "print(arr)\n",
    "x = torch.from_numpy(arr)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From another tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1,2,3,4,5])\n",
    "print(arr)\n",
    "x = torch.from_numpy(arr)\n",
    "print(x)\n",
    "#will create a tensor of ones with same dimension of x\n",
    "ones = torch.ones_like(x)\n",
    "print(ones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With random or constant values**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.rand'><strong><tt>torch.rand(size)</tt></strong></a> returns random samples from a uniform distribution over [0, 1)<br>\n",
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.randn'><strong><tt>torch.randn(size)</tt></strong></a> returns samples from the \"standard normal\" distribution [σ = 1]<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Unlike <tt>rand</tt> which is uniform, values closer to zero are more likely to appear.<br>\n",
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.randint'><strong><tt>torch.randint(low,high,size)</tt></strong></a> returns random integers from low (inclusive) to high (exclusive)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensors from uniform distribution : \n",
      " tensor([[0.7758, 0.7918, 0.3429],\n",
      "        [0.9486, 0.9792, 0.3797],\n",
      "        [0.4408, 0.6230, 0.8413],\n",
      "        [0.3331, 0.2486, 0.5846]])\n",
      "\n",
      "Random tensors from normal distribution : \n",
      " tensor([[-1.6670,  0.5261,  0.1993],\n",
      "        [-1.7312, -0.7829, -0.5817],\n",
      "        [-0.6793,  2.9097,  0.5095],\n",
      "        [ 0.4781,  0.2146, -0.7231]])\n",
      "\n",
      "Random tensors between two integers : \n",
      " tensor([[4, 5, 1, 3],\n",
      "        [4, 4, 4, 2],\n",
      "        [1, 5, 4, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_u = torch.rand(4, 3)\n",
    "x_n = torch.randn(4, 3)\n",
    "x_int = torch.randint(1, 6, (3, 4))\n",
    "\n",
    "print(f\"Random tensors from uniform distribution : \\n {x_u}\\n\")\n",
    "print(f\"Random tensors from normal distribution : \\n {x_n}\\n\")\n",
    "print(f\"Random tensors between two integers : \\n {x_int}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random number tensors that follow input size**\n",
    "\n",
    "<a href='https://pytorch.org/docs/stable/torch.html#torch.rand_like'><strong><tt>torch.rand_like(input)</tt></strong></a><br>\n",
    "<a href='https://pytorch.org/docs/stable/torch.html#torch.randn_like'><strong><tt>torch.randn_like(input)</tt></strong></a><br>\n",
    "<a href='https://pytorch.org/docs/stable/torch.html#torch.randint_like'><strong><tt>torch.randint_like(input,low,high)</tt></strong></a><br> \n",
    "<a href='https://pytorch.org/docs/stable/torch.html#torch.zeros_like'><strong><tt>torch.zeros_like(input)</tt></strong></a><br>\n",
    "<a href='https://pytorch.org/docs/stable/torch.html#torch.ones_like'><strong><tt>torch.ones_like(input)</tt></strong></a><br>\n",
    "\n",
    "these return random number tensors with the same size as <tt>input</tt>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1073, 0.9458, 0.7245, 0.0866, 0.4241],\n",
       "        [0.7996, 0.3360, 0.3151, 0.2148, 0.9967]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor of same shape of x but from uniform distribution\n",
    "u_t = torch.rand_like(x)\n",
    "u_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2965, -0.1857, -1.2174, -0.2693,  1.4423],\n",
       "        [-1.4133,  0.1271, -0.7063, -2.3060,  0.0129]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor of same shape of x but from normal distribution\n",
    "n_t = torch.randn_like(x)\n",
    "n_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27., 22., 17., 17., 29.],\n",
       "        [13., 23., 25., 28., 24.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor of same shape of x but intergers between two numbers\n",
    "n_t = torch.randint_like(x, 10, 30)\n",
    "n_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor of same shape of x but all ones\n",
    "o_t = torch.ones_like(x)\n",
    "o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor of same shape of x but all zeros\n",
    "o_t = torch.zeros_like(x)\n",
    "o_t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Datatypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"display: inline-block\">\n",
    "<tr><th>TYPE</th><th>NAME</th><th>EQUIVALENT</th><th>TENSOR TYPE</th></tr>\n",
    "<tr><td>32-bit integer (signed)</td><td>torch.int32</td><td>torch.int</td><td>IntTensor</td></tr>\n",
    "<tr><td>64-bit integer (signed)</td><td>torch.int64</td><td>torch.long</td><td>LongTensor</td></tr>\n",
    "<tr><td>16-bit integer (signed)</td><td>torch.int16</td><td>torch.short</td><td>ShortTensor</td></tr>\n",
    "<tr><td>32-bit floating point</td><td>torch.float32</td><td>torch.float</td><td>FloatTensor</td></tr>\n",
    "<tr><td>64-bit floating point</td><td>torch.float64</td><td>torch.double</td><td>DoubleTensor</td></tr>\n",
    "<tr><td>16-bit floating point</td><td>torch.float16</td><td>torch.half</td><td>HalfTensor</td></tr>\n",
    "<tr><td>8-bit integer (signed)</td><td>torch.int8</td><td></td><td>CharTensor</td></tr>\n",
    "<tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td></td><td>ByteTensor</td></tr></table>\n",
    "\n",
    "<a href='https://pytorch.org/docs/stable/tensors.html'>Tensor Datatypes</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying vs. sharing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a><br>\n",
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a><br>\n",
    "- <a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a><br>\n",
    "\n",
    "There are a number of different functions available for <a href='https://pytorch.org/docs/stable/torch.html#creation-ops'>creating tensors</a>. When using <a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a>, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the <a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a> function always makes a copy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using from_numpy \n",
      " tensor([4, 7, 9])\n",
      "Using torch.tensor \n",
      " tensor([4, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([4, 7, 9])\n",
    "\n",
    "t1 = torch.from_numpy(arr)\n",
    "t2 = torch.tensor(arr)\n",
    "\n",
    "print(f\"Using from_numpy \\n {t1}\")\n",
    "print(f\"Using torch.tensor \\n {t2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `from_numpy()` method is depecrated in favor of `tensor()`, which is a more capable method in the torch package. `from_numpy()` is only there for backwards compatibility. It can be a little bit quirky, so I recommend using the newer method in PyTorch >= 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using from_numpy \n",
      " tensor([99,  7,  9])\n",
      "Using torch.tensor \n",
      " tensor([4, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "#now modify the numpy array\n",
    "arr[0] = 99\n",
    "\n",
    "print(f\"Using from_numpy \\n {t1}\")\n",
    "print(f\"Using torch.tensor \\n {t2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"t1\" which was created using *torch.from_numpy* got changed when we modified the underlying numpy array but \"t2\" remained same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class constructors\n",
    "- <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor()</tt></strong></a><br>\n",
    "- <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.FloatTensor()</tt></strong></a><br>\n",
    "- <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.LongTensor()</tt></strong></a>\n",
    "<br>\n",
    "\n",
    "Note :\n",
    "\n",
    "- There's a subtle difference between using the factory function <font color=green><tt>torch.tensor(data)</tt></font> and the class constructor <font color=green><tt>torch.Tensor(data)</tt></font>.<br>\n",
    "- The factory function determines the dtype from the incoming data, or from a passed-in dtype argument.<br>\n",
    "- The class constructor <tt>torch.Tensor()</tt>is simply an alias for <tt>torch.FloatTensor(data)</tt>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we specified the dimensions as the arguments to the constructor by passing the numbers directly – and not a list or a tuple, which would have very different outcomes as we will see below! We can always inspect the size of the tensor using the `size()` method.\n",
    "\n",
    "The constructor method allocates space in memory for this tensor. However, the tensor is *non-initialized*. In order to initialize it, we need to call any of the tensor initialization methods of the basic tensor types. For example, the tensor we just created has a built-in method `zero_()`:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.FloatTensor\n",
      "tensor([1., 2., 3.]) torch.FloatTensor\n",
      "tensor([1, 2, 3]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "\n",
    "a = torch.Tensor(data)  # Equivalent to cc = torch.FloatTensor(data)\n",
    "print(a, a.type())\n",
    "\n",
    "c = torch.FloatTensor(data)\n",
    "print(c, c.type())\n",
    "\n",
    "b = torch.tensor(data)\n",
    "print(b, b.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor(2, 3)\n",
    "print(t)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underscore after the method name is important: it means that the operation happens _in place_: the returned object is the same object but now with different content. A very handy way to construct a tensor using the constructor happens when we have available the content we want to put in the tensor in the form of a Python iterable. In this case, we just pass it as the argument to the constructor:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now see emerging two important paradigms in PyTorch.** \n",
    "- The _imperative_ approach to performing operations, using _inplace_ methods, is in marked contrast with an additional paradigm also used in PyTorch\n",
    "- The _functional_ approach, where the returned object is a copy of the original object. \n",
    "\n",
    "Both paradigms have their specific use cases as we will be seeing below. \n",
    "\n",
    "**The rule of thumb** is that _inplace_ methods are faster and don't require extra memory allocation in general, but they can be tricky to understand (keep this in mind regarding the computational graph that we will see below). _Functional_ methods make the code referentially transparent, which is a highly desired property that makes it easier to understand the underlying math, but we rely on the efficiency of the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new copy of the tensor that is still linked to\n",
    "# the computational graph (see below)\n",
    "t1 = torch.clone(t)\n",
    "assert id(t) != id(t1), 'Functional methods create a new copy of the tensor'\n",
    "\n",
    "# To create a new _independent_ copy, we do need to detach\n",
    "# from the graph\n",
    "t1 = torch.clone(t).detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of Tensors\n",
    "\n",
    "- Shape\n",
    "- Size\n",
    "- device\n",
    "- layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,3])\n",
    "a = torch.Tensor(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**device**\n",
    "\n",
    "- PyTorch supports use of multiple <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch-device'>devices</a>, harnessing the power of one or more GPUs in addition to the CPU. \n",
    "- Operations between tensors can only happen for tensors installed on the `same device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**layout**\n",
    "\n",
    "PyTorch has a class to hold the <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.layout'>memory layout</a> option. The default setting is <a href='https://en.wikipedia.org/wiki/Stride_of_an_array'>strided</a> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations on Tensors\n",
    "\n",
    "* Indexing and slicing\n",
    "* Type Conversion\n",
    "* Reshaping tensors (tensor views)\n",
    "* Tensor arithmetic and basic operations\n",
    "* Dot products\n",
    "* Matrix multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexing and Slicing**\n",
    "\n",
    "Extracting specific values from a tensor works just the same as with NumPy arrays<br>\n",
    "<img src='../Images/arrayslicing.png' width=\"500\" style=\"display: inline-block\"><br><br>\n",
    "Image source: http://www.scipy-lectures.org/_images/numpy_indexing.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(3,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing the right hand column values\n",
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [3],\n",
       "        [5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing the right hand column as a (3,1) slice\n",
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing the left hand column as a (3,1) slice\n",
    "x[:, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also supports indexing using long tensors, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([0, 1])\n",
    "j = torch.tensor([0, 1])\n",
    "\n",
    "x[i, j]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type Conversion**\n",
    "\n",
    "Each tensor has a set of convenient methods to convert types. For example, if we want to convert the tensor above to a 32-bit float tensor, we use the method `.float()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "t = t.float()   # converts to 32-bit float\n",
    "print(t)\n",
    "t = t.double()  # converts to 64-bit float\n",
    "print(t)\n",
    "t = t.byte()    # converts to unsigned 8-bit integer\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping tensors**\n",
    "\n",
    "<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view'><strong><tt>view()</tt></strong></a> and <a href='https://pytorch.org/docs/master/torch.html#torch.reshape'><strong><tt>reshape()</tt></strong></a> do essentially the same thing by returning a reshaped tensor without changing the original tensor in place.<br>\n",
    "There's a good discussion of the differences <a href='https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch'>here</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8595,  2.1165,  0.4684],\n",
      "        [ 0.1508, -0.7908,  0.1327],\n",
      "        [ 0.2961, -0.7076, -0.1124],\n",
      "        [ 0.5378, -0.0915,  2.2167]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((4, 3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8595,  2.1165],\n",
       "        [ 0.4684,  0.1508],\n",
       "        [-0.7908,  0.1327],\n",
       "        [ 0.2961, -0.7076],\n",
       "        [-0.1124,  0.5378],\n",
       "        [-0.0915,  2.2167]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape 6, 2\n",
    "x_v = x.view(6, 2)\n",
    "x_v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : Views reflect the most current data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100e+02,  2.1165e+00],\n",
       "        [ 4.6843e-01,  1.5078e-01],\n",
       "        [-7.9075e-01,  1.3271e-01],\n",
       "        [ 2.9611e-01, -7.0765e-01],\n",
       "        [-1.1242e-01,  5.3781e-01],\n",
       "        [-9.1452e-02,  2.2167e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example\n",
    "x[0][0] = 101\n",
    "\n",
    "x_v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : Reshape also do same thing as view but torch.view can only operate on contiguous tensors, while torch.reshape can operate on both*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0111, -0.4208, -0.2684],\n",
      "        [-0.2729, -1.0976, -0.8959],\n",
      "        [ 0.5108, -0.1508,  0.5257],\n",
      "        [ 0.3561, -0.5570, -0.6945]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((4, 3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0111, -0.4208, -0.2684, -0.2729],\n",
       "        [-1.0976, -0.8959,  0.5108, -0.1508],\n",
       "        [ 0.5257,  0.3561, -0.5570, -0.6945]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r = x.reshape((3, 4))\n",
    "x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor tensor([[-2.0000e+02, -4.2084e-01, -2.6841e-01, -2.7288e-01],\n",
      "        [-1.0976e+00, -8.9587e-01,  5.1079e-01, -1.5077e-01],\n",
      "        [ 5.2568e-01,  3.5605e-01, -5.5703e-01, -6.9453e-01]])\n"
     ]
    }
   ],
   "source": [
    "#now change the first element of x\n",
    "x[0][0] = -200\n",
    "\n",
    "print(f\"Reshaped tensor {x_r}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infering correct size**\n",
    "\n",
    "By passing in <tt>-1</tt> PyTorch will infer the correct value from the given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8124,  0.2746, -1.3164],\n",
      "        [-0.1530,  0.4343,  0.2952],\n",
      "        [-0.0430, -0.1117,  0.0517],\n",
      "        [ 0.0852,  0.0904,  0.5756]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((4, 3))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8124,  0.2746, -1.3164, -0.1530,  0.4343,  0.2952, -0.0430, -0.1117,\n",
      "         0.0517,  0.0852,  0.0904,  0.5756])\n"
     ]
    }
   ],
   "source": [
    "#flatten x\n",
    "x_flatten = x.view(-1)\n",
    "print(x_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8124,  0.2746, -1.3164, -0.1530],\n",
      "        [ 0.4343,  0.2952, -0.0430, -0.1117],\n",
      "        [ 0.0517,  0.0852,  0.0904,  0.5756]])\n"
     ]
    }
   ],
   "source": [
    "#flatten x\n",
    "x_reshaped = x.view(-1, 4)\n",
    "print(x_reshaped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-1` input of `view()` in above operation got value of `3`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adopt another tensor's shape with <tt>.view_as()</tt>**\n",
    "\n",
    "<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view_as'><strong><tt>view_as(input)</tt></strong></a> only works with tensors that have the same number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low = 10, high=20, size=(4, 5))\n",
    "\n",
    "z = torch.randint(0, 10, (20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 0, 9, 5],\n",
       "        [1, 7, 9, 9, 0],\n",
       "        [3, 5, 8, 2, 3],\n",
       "        [4, 9, 0, 8, 9]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view z as x\n",
    "z.view_as(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it reshaped `z` into share of input tensor `x`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Arithmetic**\n",
    "\n",
    "- Addition\n",
    "- Substraction\n",
    "- Dot products\n",
    "- Matrix multiplication\n",
    "- Broadcasting\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Addition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3], dtype=torch.float)\n",
    "b = torch.tensor([4,5,6], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3., -3., -3.])\n"
     ]
    }
   ],
   "source": [
    "print(a+b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition is happening elementwise. We can also use `torch.add(a, b)` for addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "s = torch.add(a, b)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Substraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3], dtype=torch.float)\n",
    "b = torch.tensor([4,5,6], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(b-a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as addition. `torch.substract(a, b)` can also be used for same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "d = torch.subtract(b, a)\n",
    "print(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a dummy tensor in `torch.add()` method to get the output of the resultant addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 17., 11., 12., 13.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(1, 5, (5,))\n",
    "b = torch.randint(10, 15, (5,))\n",
    "\n",
    "#create a dummy variable of same size of a\n",
    "dummy = torch.empty((5,))\n",
    "torch.add(a, b, out=dummy)\n",
    "\n",
    "print(dummy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Changing a tensor in-place*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 13, 15, 15, 14])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(1, 5, (5,))\n",
    "b = torch.randint(10, 15, (5,))\n",
    "\n",
    "a.add_(b) # equivalent to a=torch.add(a,b)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> Any operation that changes a tensor in-place is post-fixed with an underscore _.\n",
    "    <br>In the above example: <tt>a.add_(b)</tt> changed <tt>a</tt>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Tensor Operations\n",
    "<table style=\"display: inline-block\">\n",
    "<caption style=\"text-align: center\"><strong>Arithmetic</strong></caption>\n",
    "<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>a + b</td><td>a.add(b)</td><td>element wise addition</td></tr>\n",
    "<tr><td>a - b</td><td>a.sub(b)</td><td>subtraction</td></tr>\n",
    "<tr><td>a * b</td><td>a.mul(b)</td><td>multiplication</td></tr>\n",
    "<tr><td>a / b</td><td>a.div(b)</td><td>division</td></tr>\n",
    "<tr><td>a % b</td><td>a.fmod(b)</td><td>modulo (remainder after division)</td></tr>\n",
    "<tr><td>a<sup>b</sup></td><td>a.pow(b)</td><td>power</td></tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"display: inline-block\">\n",
    "<caption style=\"text-align: center\"><strong>Trigonometry</strong></caption>\n",
    "<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>sin(a)</td><td>torch.sin(a)</td><td>sine</td></tr>\n",
    "<tr><td>cos(a)</td><td>torch.sin(a)</td><td>cosine</td></tr>\n",
    "<tr><td>tan(a)</td><td>torch.sin(a)</td><td>tangent</td></tr>\n",
    "<tr><td>arcsin(a)</td><td>torch.asin(a)</td><td>arc sine</td></tr>\n",
    "<tr><td>arccos(a)</td><td>torch.acos(a)</td><td>arc cosine</td></tr>\n",
    "<tr><td>arctan(a)</td><td>torch.atan(a)</td><td>arc tangent</td></tr>\n",
    "<tr><td>sinh(a)</td><td>torch.sinh(a)</td><td>hyperbolic sine</td></tr>\n",
    "<tr><td>cosh(a)</td><td>torch.cosh(a)</td><td>hyperbolic cosine</td></tr>\n",
    "<tr><td>tanh(a)</td><td>torch.tanh(a)</td><td>hyperbolic tangent</td></tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"display: inline-block\">\n",
    "<caption style=\"text-align: center\"><strong>Summary Statistics</strong></caption>\n",
    "<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>$\\sum a$</td><td>torch.sum(a)</td><td>sum</td></tr>\n",
    "<tr><td>$\\bar a$</td><td>torch.mean(a)</td><td>mean</td></tr>\n",
    "<tr><td>a<sub>max</sub></td><td>torch.max(a)</td><td>maximum</td></tr>\n",
    "<tr><td>a<sub>min</sub></td><td>torch.min(a)</td><td>minimum</td></tr>\n",
    "<tr><td colspan=\"3\">torch.max(a,b) returns a tensor of size a<br>containing the element wise max between a and b</td></tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> Most arithmetic operations require float values. Those that do work with integers return integer tensors.<br>\n",
    "For example, <tt>torch.div(a,b)</tt> performs floor division (truncates the decimal) for integer types, and classic division for floats.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3], dtype=torch.float)\n",
    "b = torch.tensor([4,5,6], dtype=torch.float)\n",
    "print(torch.add(a,b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(a,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.max(a,b)` returns a tensor of size a containing the element wise max between a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot products**\n",
    "\n",
    "A <a href='https://en.wikipedia.org/wiki/Dot_product'>dot product</a> is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as:<br>\n",
    "\n",
    "$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf$\n",
    "\n",
    "If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example:<br>\n",
    "$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf$<br><br>\n",
    "Dot products can be expressed as <a href='https://pytorch.org/docs/stable/torch.html#torch.dot'><strong><tt>torch.dot(a,b)</tt></strong></a> or `a.dot(b)` or `b.dot(a)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor([4., 5., 6.]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3], dtype=torch.float)\n",
    "b = torch.tensor([4,5,6], dtype=torch.float)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "a_dot_b = torch.dot(a, b)\n",
    "\n",
    "print(a_dot_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> There's a slight difference between <tt>torch.dot()</tt> and <tt>numpy.dot()</tt>. While <tt>torch.dot()</tt> only accepts 1D arguments and returns a dot product, <tt>numpy.dot()</tt> also accepts 2D arguments and performs matrix multiplication</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix multiplication**\n",
    "\n",
    "2D <a href='https://en.wikipedia.org/wiki/Matrix_multiplication'>Matrix multiplication</a> is possible when the number of columns in tensor <strong><tt>A</tt></strong> matches the number of rows in tensor <strong><tt>B</tt></strong>. In this case, the product of tensor <strong><tt>A</tt></strong> with size $(x,y)$ and tensor <strong><tt>B</tt></strong> with size $(y,z)$ results in a tensor of size $(x,z)$\n",
    "<div>\n",
    "<div align=\"left\"><img src='../Images/Matrix_multiplication_diagram.png' align=\"left\"><br><br>\n",
    "\n",
    "$\\begin{bmatrix} a & b & c \\\\\n",
    "d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\\n",
    "(dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$</div></div>\n",
    "\n",
    "<div style=\"clear:both\">Image source: <a href='https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg'>https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg</a></div>\n",
    "\n",
    "Matrix multiplication can be computed using <a href='https://pytorch.org/docs/stable/torch.html#torch.mm'><strong><tt>torch.mm(a,b)</tt></strong></a> or `a.mm(b)` or `a @ b` or `a.matmul(b)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  torch.Size([2, 3])\n",
      "b:  torch.Size([3, 2])\n",
      "a x b:  torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0,2,4],[1,3,5]], dtype=torch.float)\n",
    "b = torch.tensor([[6,7],[8,9],[10,11]], dtype=torch.float)\n",
    "\n",
    "print('a: ',a.size())\n",
    "print('b: ',b.size())\n",
    "print('a x b: ',torch.mm(a,b).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[56., 62.],\n",
       "         [80., 89.]]),\n",
       " tensor([[56., 62.],\n",
       "         [80., 89.]]),\n",
       " tensor([[56., 62.],\n",
       "         [80., 89.]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix multiplications\n",
    "c = torch.matmul(a, b)\n",
    "c1 = a @ b\n",
    "c2 = a.matmul(b)\n",
    "c, c1, c2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `torch.mm` can not be used in case we need broadcasting\n",
    "\n",
    "**Matrix multiplication with broadcasting**\n",
    "\n",
    "Matrix multiplication that involves <a href='https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics'>broadcasting</a> can be computed using <a href='https://pytorch.org/docs/stable/torch.html#torch.matmul'><strong><tt>torch.matmul(a,b)</tt></strong></a> or `a.matmul(b)` or `a @ b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6263, -0.4267,  1.6157,  0.7315],\n",
      "         [-1.1204, -1.7563, -0.2813, -0.6717],\n",
      "         [-1.2577, -0.7797, -0.1162, -0.9286]],\n",
      "\n",
      "        [[ 1.4452, -0.9090, -0.2240, -0.0716],\n",
      "         [ 0.1560, -0.3195,  0.5765, -0.0261],\n",
      "         [-0.1909, -0.3241,  1.1457, -0.1089]]])\n",
      "tensor([[-0.3800,  2.9753, -0.9240,  0.6155,  0.3324],\n",
      "        [ 1.0023, -0.5933,  1.1221, -0.2481, -1.2043],\n",
      "        [ 0.8274, -1.5916, -1.0329, -0.3710, -0.6825],\n",
      "        [ 1.1768,  1.5561, -0.5304, -0.1461,  1.3011]])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn(2, 3, 4)\n",
    "t2 = torch.randn(4, 5)\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(torch.matmul(t1, t2).size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the same operation raises a <tt><strong>RuntimeError</strong></tt> with <tt>torch.mm()</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ajitkumarsingh/Desktop/Hands-on-with-PyTorch/Basics of PyTorch/PyTorch's Basics.ipynb Cell 121\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ajitkumarsingh/Desktop/Hands-on-with-PyTorch/Basics%20of%20PyTorch/PyTorch%27s%20Basics.ipynb#Y235sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mmm(t1, t2)\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "print(torch.mm(t1, t2).size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance operations over Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 or Euclidian Norm**\n",
    "\n",
    "See <a href='https://pytorch.org/docs/stable/torch.html#torch.norm'><strong><tt>torch.norm()</tt></strong></a>\n",
    "\n",
    "The <a href='https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm'>Euclidian Norm</a> gives the vector norm of $x$ where $x=(x_1,x_2,...,x_n)$.<br>\n",
    "It is calculated as<br>\n",
    "\n",
    "${\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}$\n",
    "\n",
    "\n",
    "When applied to a matrix, <tt>torch.norm()</tt> returns the <a href='https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm'>Frobenius norm</a> by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.,5.,8.,14.])\n",
    "x.norm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of elements**\n",
    "\n",
    "See <a href='https://pytorch.org/docs/stable/torch.html#torch.numel'><strong><tt>torch.numel()</tt></strong></a>\n",
    "\n",
    "Returns the number of elements in a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z : tensor([[13, 13, 17,  9,  6],\n",
      "        [ 9, 17, 17,  3,  6],\n",
      "        [18,  6, 15,  1,  2],\n",
      "        [10,  6,  3,  6, 17]])\n",
      "Total Number of elements in z are : 20\n"
     ]
    }
   ],
   "source": [
    "z = torch.randint(low=1, high=20, size=(4, 5))\n",
    "print(f\"z : {z}\")\n",
    "\n",
    "print(f\"Total Number of elements in z are : {z.numel()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU computation\n",
    "\n",
    "- Deep Learning frameworks take advantage of the powerful computational capabilities of modern graphic processing units (GPUs). GPUs were originally designed to perform frequent operations for graphics very efficiently and fast, such as linear algebra operations, which makes them ideal for our interests. \n",
    "- PyTorch makes it very easy to use the GPU: \n",
    "    - the common scenario is to tell the framework that we want to instantiate a tensor with a type that makes it a GPU tensor\n",
    "    -  or move a given CPU tensor to the GPU. All the tensors that we have seen above are CPU tensors, and PyTorch has the counterparts for GPU tensors in the `torch.cuda` module. Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t_gpu = torch.cuda.FloatTensor(3, 3)   # creation of a GPU tensor\n",
    "    t_gpu.zero_()                          # initialization to zero\n",
    "except TypeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more common approach that gives us flexibility is through the use of devices.\n",
    "\n",
    "A device in PyTorch refers to either the CPU (indicated by the string \"cpu\") or one of the possible GPU cards in the machine (indicated by the string \"cuda:$n$\", where $n$ is the index of the card). \n",
    "\n",
    "Let's create a random gaussian matrix using a method from the `torch` package, and set the computational device to be the GPU by specifying the `device` to be `cuda:0`, the first GPU card in our machine (this code will fail if you don't have a GPU, but we will work around that below): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t_gpu = torch.randn(3, 3, device=\"cuda:0\")\n",
    "except AssertionError as err:\n",
    "    print(err)\n",
    "    t_gpu = None\n",
    "\n",
    "t_gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, the tensor now has the explicit device set to be a CUDA device, not a CPU device. Let's now create a tensor in the CPU and move it to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6876,  0.1021,  0.0404],\n",
       "        [ 0.5393, -0.5799,  1.3055],\n",
       "        [-2.2185, -1.0130,  0.9707]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could also state explicitly the device to be the\n",
    "# CPU with torch.randn(3,3,device=\"cpu\")\n",
    "t = torch.randn(3, 3)\n",
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the device is the CPU, but PyTorch does not explicitly say that given that this is the default behavior. To copy the tensor to the GPU we use the `.to()` method that every tensor implements, passing the device as an argument. This method creates a copy in the specified device or, if the tensor already resides in that device, it returns the original tensor ([Lapan 2018](#References)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t_gpu = t.to(\"cuda:0\")  # copies the tensor from CPU to GPU\n",
    "    # note that if we do now t_to_gpu.to(\"cuda:0\") it will\n",
    "    # return the same tensor without doing anything else\n",
    "    # as this tensor already resides on the GPU\n",
    "    print(t_gpu)\n",
    "    print(t_gpu.device)\n",
    "except AssertionError as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: When we program PyTorch models, we will have to specify the device in several places (not so many, but definitely more than once). A good practice that is consistent accross the implementation and makes the code more portable is to declare early in the code a device variable by querying the framework if there is a GPU available that we can use. We can do this by writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `device` as an argument of the `.to()` method in the rest of our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6876,  0.1021,  0.0404],\n",
       "        [ 0.5393, -0.5799,  1.3055],\n",
       "        [-2.2185, -1.0130,  0.9707]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moves t to the device (this code will **not** fail if the\n",
    "# local machine has not access to a GPU)\n",
    "t.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: having good GPU backend support is a critical aspect of a deep learning framework. Some models depend crucially on performing computations on a GPU. Most frameworks, including PyTorch, only provide good support for GPUs manufactured by Nvidia. This is mostly due to the heavy investment this company made on CUDA (Compute Unified Device Architecture), the underlying parallel computing platform that enables this type of scientific computing (and the reason for the device label), with specific implementations targeted to Deep Neural Networks as cuDNN. Other GPU manufacturers, most notably AMD, are making efforts to towards enabling ML computing in their cards, but their support is still partial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Foundations\n",
    "\n",
    "Computing gradients is a crucial feature in deep learning, given that the training procedure of neural networks relies on optimization techniques that update the parameters of the model by using the gradient information of a scalar magnitude – the loss function. \n",
    "\n",
    "How is it possible to compute the derivatives? There are different methods, namely\n",
    "\n",
    "- **Symbolic Differentiation**: given a symbolic expression, the software provides the derivative by performing symbolic transformations (e.g. Wolfram Alpha). The benefits are clear, but it is not always possible to compute an analytical expression.\n",
    "\n",
    "- **Numerical Differentiation**: computes the derivatives using expressions that are suitable to be evaluated numerically, using the finite differences method to several orders of approximation. A big drawback is that these methods are slow.\n",
    "\n",
    "- **Automatic Differentiation**: a library adds to the set of functional primitives an implementation of the derivative for each of these functions. Thus, if the library contains the function $sin(x)$, it also implements the derivative of this function, $\\frac{d}{dx}sin(x) = cos(x)$. Then, given a composition of functions, the library can compute the derivative with respect a variable by successive application of the chain rule, a method that is known in deep learning as backpropagation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd - Automatic Differentiation\n",
    "\n",
    "Modern deep learning libraries are capable of performing automatic differentiation. The two main approaches to computing the graph are _static_ and _dynamic_ processing :\n",
    "\n",
    "- **Static graphs**: the deep learning framework converts the computational graph into a static representation that cannot be modified. This allows the library developers to do very aggressive optimizations on this static graph ahead of computation time, pruning some areas and transforming others so that the final product is highly optimized and fast. The drawback is that some models can be really hard to implement with this approach. For example, TensorFlow uses static graphs. Having static graphs is part of the reason why TensorFlow has excellent support for sequence processing, which makes it very popular in NLP.\n",
    "\n",
    "- **Dynamic graphs**: the framework does not create a graph ahead of computation, but records the operations that are performed, which can be quite different for different inputs. When it is time to compute the gradients, it unrolls the graph and perform the computations. A major benefit of this approach is that implementing complex models can be easier in this paradigm. This flexibility comes at the expense of the major drawback of this approach: speed. Dynamic graphs cannot leverage the same level of ahead-of-time optimization as static graphs, which makes them slower. PyTorch uses dynamic graphs as the underlying paradigm for gradient computation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is simple graph to compute $y = wx + b$ \n",
    "\n",
    "<img src=\"fig/simple_computation_graph.png\" width=500 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch computes the graph using the Autograd system. Autograd records a graph when performing the forward pass (function application), keeping track of all the tensors defined as inputs. These are the leaves of the graph. The output tensors are the roots of the graph. By navigating this graph from root to leaves, the gradients are automatically computed using the chain rule. In summary,\n",
    "\n",
    "- Forward pass (the successive function application) goes from leaves to root. We use the `apply` method in PyTorch.\n",
    "- Once the forward pass is completed, Autograd has recorded the graph and the backward pass (chain rule) can be done. We use the method `backwards` on the root of the graph.\n",
    "\n",
    "This section covers the PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> implementation of gradient descent. Tools include:\n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward'><tt><strong>torch.autograd.backward()</strong></tt></a>\n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad'><tt><strong>torch.autograd.grad()</strong></tt></a>\n",
    "\n",
    "<div class=\"alert alert-info\"><p>Additional Resources:</p>\n",
    "<strong>\n",
    "<a href='https://pytorch.org/docs/stable/notes/autograd.html'>PyTorch Notes:</a></strong>&nbsp;&nbsp;<font color=green>Autograd mechanics</font></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> package provides automatic differentiation for all operations on Tensors. \n",
    "- This is because operations become attributes of the tensors themselves. When a Tensor's <tt>.requires_grad</tt> attribute is set to True, it starts to track all operations on it.\n",
    "- When an operation finishes you can call <tt>.backward()</tt> and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its <tt>.grad</tt> attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back-propagation**\n",
    "\n",
    "On single dependencies\n",
    "\n",
    "We'll start by applying a single polynomial function $y = f(x)$ to tensor $x$. Then we'll backprop and print the gradient $\\frac {dy} {dx}$.\n",
    "\n",
    "\n",
    "Given function:\n",
    " y = 2x^4 + x^3 + 3x^2 + 5x + 1 \n",
    "\n",
    "Derivative:\n",
    " y' = 8x^3 + 3x^2 + 6x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor with requires_grad = True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#deifne the above function\n",
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $y$ was created as a result of an operation, it has an associated gradient function accessible as <tt>y.grad_fn</tt><br>\n",
    "The calculation of $y$ is done as:<br>\n",
    "\n",
    "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\n",
    "\n",
    "This is the value of $y$ when $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use .backward to evaluate the gradient\n",
    "y.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display resultant gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "source": [
    "x_grad = x.grad\n",
    "print(x_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <tt>x.grad</tt> is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of<br>\n",
    "\n",
    "$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$\n",
    "\n",
    "This is the slope of the polynomial at the point $(2,63)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation on multisteps**\n",
    "\n",
    "Now let's do something more complex, involving layers $y$ and $z$ between $x$ and output layer $out$.\n",
    "\n",
    "$\\quad y = 3x+2$\n",
    "\n",
    "$\\quad z = 2y^2$\n",
    "\n",
    "$\\quad out = mean(x)$\n",
    "\n",
    "*Need to find derivative of out w.r.t x*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 3*x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*y**2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = torch.mean(z)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform backward popagation over $out$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "source": [
    "#check the gradient\n",
    "x_grad = x.grad\n",
    "print(x_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a 2x3 matrix. If we call the final <tt>out</tt> tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows:<br>\n",
    "\n",
    "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
    "\n",
    "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
    "\n",
    "To solve the derivative of $z_i$ we use the <a href='https://en.wikipedia.org/wiki/Chain_rule'>chain rule</a>, where the derivative of $f(g(x)) = f'(g(x))g'(x)$<br>\n",
    "\n",
    "In this case<br>\n",
    "\n",
    "$\\begin{align} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\\\\n",
    "g(x) &= 3x+2, &g'(x) = 3 \\\\\n",
    "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{align}$\n",
    "\n",
    "Therefore,<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning off tracking**\n",
    "\n",
    "There may be times when we don't want or need to track the computational history.\n",
    "\n",
    "You can reset a tensor's <tt>requires_grad</tt> attribute in-place using `.requires_grad_(True)` (or False) as needed.\n",
    "\n",
    "When performing evaluations, it's often helpful to wrap a set of operations in `with torch.no_grad():`\n",
    "\n",
    "A less-used method is to run `.detach()` on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader\n",
    "\n",
    "- PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. \n",
    "- Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a custom dataset for your files**\n",
    "\n",
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using DataLoaders**\n",
    "\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "```python\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "\n",
    "In this section I will be implementing a neural network architecture using PyTorch functionalities and will be showing several common methods and functions that we use as a fundamental component of a neural network like loss functions, activation functions etc.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- torch.nn Package\n",
    "- Linear layers\n",
    "- Activation fuctions\n",
    "- Loss function\n",
    "- Optimization\n",
    "- Model's traning\n",
    "- Model's evaluation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "The base implementation for all neural network models in PyTorch is the class `Module` in the package `torch.nn`:\n",
    "\n",
    "````\n",
    "import torch.nn as nn\n",
    "\n",
    "````\n",
    "All our models subclass this base `nn.Module` class, which provides an interface to important methods used for constructing and working with our models, and which contains sensible initializations for our models. Modules can contain other modules (and usually do).\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{U}(f(\\mathbf{W}(\\mathbf{x})))$$\n",
    "\n",
    "where $f$ is a non-linear function (a `ReLU`), is directly translated into a similar expression in PyTorch. To do that, we simply subclass `nn.Module`, register the two affine transformations and the non-linearity, and implement their composition within the `forward` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomModule(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output_classes):\n",
    "        # call super to initialize the class above in the hierarchy\n",
    "        super(MyCustomModule, self).__init__()\n",
    "        # first affine transformation\n",
    "        self.W = nn.Linear(n_inputs, n_hidden)\n",
    "        # non-linearity (here it is also a layer!)\n",
    "        self.f = nn.ReLU()\n",
    "        # final affine transformation\n",
    "        self.U = nn.Linear(n_hidden, n_output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.U(self.f(self.W(x)))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use our new module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2863, -0.3074]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set the network's architectural parameters\n",
    "n_inputs = 3\n",
    "n_hidden= 4\n",
    "n_output_classes = 2\n",
    "\n",
    "# instantiate the model\n",
    "model = MyCustomModule(n_inputs, n_hidden, n_output_classes)\n",
    "\n",
    "# create a simple input tensor\n",
    "# size is [1,3]: a mini-batch of one example,\n",
    "# this example having dimension 3\n",
    "x = torch.FloatTensor([[0.3, 0.8, -0.4]])\n",
    "\n",
    "# compute the model output by **applying** the input to the module\n",
    "y = model(x)\n",
    "\n",
    "# inspect the output\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the output is a tensor with its gradient function attached – Autograd tracks it for us.\n",
    "\n",
    "**Tip**: modules overrides the `__call__()` method, where the framework does some work. Thus, instead of directly calling the `forward()` method, we apply the input to the model instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "A powerful class in the `nn` package is `Sequential`, which allows us to express the code above more succinctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomModule(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output_classes):\n",
    "        super(MyCustomModule, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, this can be handy when we have a large number of layers for which the actual names are not that meaningful. It also improves readability:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important package in `torch.nn` is `Functional`, typically imported as `F`. Functional contains many useful functions, from non-linear activations to convolutional, dropout, and even distance functions. Many of these functions have counterpart implementations as layers in the `nn` package so that they can be easily used in pipelines like the one above implemented using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 5.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = F.relu(torch.FloatTensor([[-5, -1, 0, 5]]))\n",
    "\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion and loss functions\n",
    "\n",
    "PyTorch has implementations for the most common criteria in the `torch.nn` package. You may notice that, as with many of the other functions, there are two implementations of loss functions: the reference functions in `torch.nn.functional` and practical class in `torch.nn`, which are the ones we typically use. Probably the two most common ones are ([Lapan 2018](#References)):\n",
    "\n",
    "- `nn.MSELoss` (mean squared error): squared $L_2$ norm used for regression.\n",
    "- `nn.CrossEntropyLoss`: criterion used for classification as the result of combining `nn.LogSoftmax()` and `nn.NLLLoss()` (negative log likelihood), operating on the input scores directly. When possible, we recommend using this class instead of using a softmax layer plus a log conversion and `nn.NLLLoss`, given that the `LossSoftmax` implementation guards against common numerical errors, resulting in less instabilities.\n",
    "\n",
    "Once our model produces a prediction, we pass it to the criteria to obtain a measure of the loss:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0335, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# the true label (in this case, 2) from our dataset wrapped\n",
    "# as a tensor of minibatch size of 1\n",
    "y_gold = torch.tensor([1])\n",
    "\n",
    "# our simple classification criterion for this simple example\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# forward pass of our model (remember, using apply instead of forward)\n",
    "y = model(x)\n",
    "\n",
    "# apply the criterion to get the loss corresponding to the pair (x, y)\n",
    "# with respect to the real y (y_gold)\n",
    "loss = criterion(y, y_gold)\n",
    "\n",
    "\n",
    "# the loss contains a gradient function that we can use to compute\n",
    "# the gradient dL/dw (gradient with respect to the parameters\n",
    "# for a given fixed input)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Once we have computed the loss for a training example or minibatch of examples, we update the parameters of the model guided by the information contained in the gradient. The role of updating the parameters belongs to the optimizer, and PyTorch has a number of implementations available right away – and if you don't find your preferred optimizer as part of the library, chances are that you will find an existing implementation. Also, coding your own optimizer is indeed quite easy in PyTorch.\n",
    "\n",
    "**Side Note** The following is a summary of the most common optimizers. It is intended to serve as a reference (I use this table myself quite a lot). In practice, most people pick an optimizer that has been proven to behave well on a given domain, but optimizers are also a very active area of research on numerical analysis, so it is a good idea to pay some attention to this subfield. We recommend using second-order dynamics with an adaptive time step:\n",
    "\n",
    "- First-order dynamics\n",
    "    - Search direction only: `optim.SGD`\n",
    "    - Adaptive: `optim.RMSprop`, `optim.Adagrad`, `optim.Adadelta`\n",
    "    \n",
    "- Second-order dynamics\n",
    "    - Search direction only: Momentum `optim.SGD(momentum=0.9)`, Nesterov, `optim.SGD(nesterov=True)`\n",
    "    - Adaptive: `optim.Adam`, `optim.Adamax` (Adam with $L_\\infty$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model's architecture looks similar to this:**\n",
    "\n",
    "<img src='images/main-qimg-a02a06108f914443fd28b925c3b998d5-pjlq.jpeg' width='400' style=\"display: inline-block\"><br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Here, I will be creating a linear fucntion as below and will try to see if neural network is able to catch the original distribution.\n",
    "\n",
    "<img src='../Images/linear-regression-residuals.png' width='400' style=\"display: inline-block\"><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll develop a collection of data points that appear random, but that fit a known linear equation $y = 2x+1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # we'll use this a lot going forward!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor of shape $50X1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.],\n",
       "        [14.],\n",
       "        [15.],\n",
       "        [16.],\n",
       "        [17.],\n",
       "        [18.],\n",
       "        [19.],\n",
       "        [20.],\n",
       "        [21.],\n",
       "        [22.],\n",
       "        [23.],\n",
       "        [24.],\n",
       "        [25.],\n",
       "        [26.],\n",
       "        [27.],\n",
       "        [28.],\n",
       "        [29.],\n",
       "        [30.],\n",
       "        [31.],\n",
       "        [32.],\n",
       "        [33.],\n",
       "        [34.],\n",
       "        [35.],\n",
       "        [36.],\n",
       "        [37.],\n",
       "        [38.],\n",
       "        [39.],\n",
       "        [40.],\n",
       "        [41.],\n",
       "        [42.],\n",
       "        [43.],\n",
       "        [44.],\n",
       "        [45.],\n",
       "        [46.],\n",
       "        [47.],\n",
       "        [48.],\n",
       "        [49.],\n",
       "        [50.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.linspace(start=1, steps=50, end=50)\n",
    "#reshape it as 50by1\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give a real feel touch add an error term as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-48.)\n"
     ]
    }
   ],
   "source": [
    "#set manual seed to give reproducable result\n",
    "torch.manual_seed(42)\n",
    "\n",
    "err = torch.randint(-8, 9, (50, 1), dtype=torch.float32)\n",
    "\n",
    "print(err.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column matric of $y$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "y = 2*X + 1 + err\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we created tensor $X$, we did <em>not</em> pass <tt>requires_grad=True</tt>. This means that $y$ doesn't have a gradient function, and <tt>y.backward()</tt> won't work. Since PyTorch is not tracking operations, it doesn't know the relationship between $X$ and $y$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Linear Model**\n",
    "\n",
    "PyTorch has built-in <tt>nn.Linear()</tt> model preselects weight and bias values at random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7645]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8300], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Linear(in_features=1, out_features=1, dtype=torch.float)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without seeing any data, the model sets a random weight of 0.7645 and a bias of 0.8300."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch lets us define models as object classes that can store multiple model layers. For now, though, we only need a single <tt>linear</tt> layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> The \"Linear\" model layer used here doesn't really refer to linear regression. Instead, it describes the type of neural network layer employed. Linear layers are also called \"fully connected\" or \"dense\" layers. Going forward our models may contain linear layers, convolutional layers, and more.</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When <tt>Model</tt> is instantiated, we need to pass in the size (dimensions) of the incoming and outgoing features. For our purposes we'll use (1,1).<br>As above, we can see the initial hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Weight: -0.23427248001098633\n",
      "Bias:   0.9186112880706787\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(1, 1)\n",
    "\n",
    "print(model)\n",
    "print('Weight:', model.linear.weight.item())\n",
    "print('Bias:  ', model.linear.bias.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model has just one layer so it is easy to get weight and biases but when model become more complex it is better to iterate over all the model parameters using `model.named_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight \t -0.23427248001098633\n",
      "linear.bias \t 0.9186112880706787\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\n",
    "    print(name, \"\\t\", param.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> In the above example we had our Model class accept arguments for the number of input and output features.<br><i>For simplicity we can hardcode them into the Model:</i>\n",
    "         \n",
    "<tt><font color=green>\n",
    "class Model(torch.nn.Module):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.linear = Linear(1,1)<br><br>\n",
    "model = Model()\n",
    "</font></tt><br><br>\n",
    "\n",
    "Alternatively we can use default arguments:\n",
    "\n",
    "<tt><font color=green>\n",
    "class Model(torch.nn.Module):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self, in_dim=1, out_dim=1):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.linear = Linear(in_dim,out_dim)<br><br>\n",
    "model = Model()<br>\n",
    "<em>\\# or</em><br>\n",
    "model = Model(i,o)</font></tt>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward**\n",
    "\n",
    "Let's create a dummy variable and pass it to `forward` method of `SimpleModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4501], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float32)\n",
    "\n",
    "#feed forward\n",
    "print(model.forward(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is confirmed with $f(x) = (-0.2342724)(2.0)+(0.918) = 0.4501$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we created the Linear layers in `SimpleModel` class, PyTorch automatically assigned some weights and bias randomly. Intial model is just like a random estimator since it has not seen the real data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot the initial model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. 50.]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([X.min(),X.max()])\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight : -0.23427248, Initial bias : 0.91861129\n",
      "[  0.6843388 -10.795012 ]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.linear.weight.item(), model.linear.bias.item()\n",
    "\n",
    "print(f\"Initial weight : {W1:.8f}, Initial bias : {b1:.8f}\")\n",
    "\n",
    "y1 = x1*W1 + b1\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb5c0c886a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(x1,y1,'r')\n",
    "plt.title('Initial Model')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend([\"Original Distribution\",\"Predicted Distribution\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the random model is far worse and we need to train the model over some data to minimize the gap between predicted distribution and original distribution.\n",
    "\n",
    "Now we need to set loss function so that the model can learn to minimize the loss and hence predicting the distribution close to original distribution.\n",
    "\n",
    "This is a regression problem so we can use metrics used for regression problems like Mean Squared Error(MSE) etc.\n",
    "\n",
    "**loss function**\n",
    "We could write our own function to apply a Mean Squared Error (MSE) that follows<br>\n",
    "\n",
    "$\\begin{align}MSE &= \\frac {1} {n} \\sum_{i=1}^n {(y_i - \\hat y_i)}^2 \\\\\n",
    "&= \\frac {1} {n} \\sum_{i=1}^n {(y_i - (wx_i + b))}^2\\end{align}$<br>\n",
    "\n",
    "Fortunately PyTorch has it built in.<br>\n",
    "<em>By convention, you'll see the variable name \"criterion\" used, but feel free to use something like \"linear_loss_func\" if that's clearer.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the optimization**\n",
    "\n",
    "- Here we'll use <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'>Stochastic Gradient Descent</a> (SGD) with an applied <a href='https://en.wikipedia.org/wiki/Learning_rate'>learning rate</a> (lr) of 0.001. \n",
    "- Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. \n",
    "- Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge.\n",
    "\n",
    "- For more complicated (multivariate) data, you might also consider passing optional <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum'><tt>momentum</tt></a> and <a href='https://en.wikipedia.org/wiki/Tikhonov_regularization'><tt>weight_decay</tt></a> arguments.\n",
    "-  Momentum allows the algorithm to \"roll over\" small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases.\n",
    "\n",
    "For more information, see <a href='https://pytorch.org/docs/stable/optim.html'><strong><tt>torch.optim</tt></strong></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model's Traning**\n",
    "\n",
    "Training consists of following steps:\n",
    "\n",
    "\n",
    "1. Set a reasonably large number of passes<br>\n",
    "<tt><font color=white>epochs = 50</font></tt><br>\n",
    "2. Create a list to store loss values. This will let us view our progress afterward.<br>\n",
    "<tt><font color=white>losses = []</font></tt><br>\n",
    "<tt><font color=white>for i in range(epochs):</font></tt><br>\n",
    "3. Bump \"i\" so that the printed report starts at 1<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;i+=1</font></tt><br>\n",
    "4. Create a prediction set by running \"X\" through the current model parameters<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;y_pred = model.forward(X)</font></tt><br>\n",
    "5. Calculate the loss<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;loss = criterion(y_pred, y)</font></tt><br>\n",
    "6. Add the loss value to our tracking list<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;losses.append(loss)</font></tt><br>\n",
    "7. Print the current line of results<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;print(f'epoch: {i:2} loss: {loss.item():10.8f}')</font></tt><br>\n",
    "8. Gradients accumulate with every backprop. To prevent compounding we need to reset the stored gradient for each new epoch.<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad()</font></tt><br>\n",
    "9. Now we can backprop<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;loss.backward()</font></tt><br>\n",
    "10. Finally, we can update the hyperparameters of our model<br>\n",
    "<tt><font color=white>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()</font></tt>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss: 4211.73388672  weight: -0.23427248  bias: 0.91861129\n",
      "epoch:  1  loss: 2184.10229492  weight: 3.55968428  bias: 1.03080201\n",
      "epoch:  2  loss: 1137.30786133  weight: 0.83369541  bias: 0.94927651\n",
      "epoch:  3  loss: 596.88525391  weight: 2.79238725  bias: 1.00693953\n",
      "epoch:  4  loss: 317.88449097  weight: 1.38506413  bias: 0.96459389\n",
      "epoch:  5  loss: 173.84628296  weight: 2.39627504  bias: 0.99410641\n",
      "epoch:  6  loss: 99.48413086  weight: 1.66973126  bias: 0.97198814\n",
      "epoch:  7  loss: 61.09345245  weight: 2.19179130  bias: 0.98696786\n",
      "epoch:  8  loss: 41.27357101  weight: 1.81671023  bias: 0.97529256\n",
      "epoch:  9  loss: 31.04116249  weight: 2.08623886  bias: 0.98276973\n",
      "epoch: 10  loss: 25.75837898  weight: 1.89260542  bias: 0.97648603\n",
      "epoch: 11  loss: 23.03093910  weight: 2.03176117  bias: 0.98009020\n",
      "epoch: 12  loss: 21.62270546  weight: 1.93180275  bias: 0.97659022\n",
      "epoch: 13  loss: 20.89556694  weight: 2.00365138  bias: 0.97819507\n",
      "epoch: 14  loss: 20.52002907  weight: 1.95205402  bias: 0.97613245\n",
      "epoch: 15  loss: 20.32601929  weight: 1.98915446  bias: 0.97670543\n",
      "epoch: 16  loss: 20.22571754  weight: 1.96252429  bias: 0.97538513\n",
      "epoch: 17  loss: 20.17380524  weight: 1.98168552  bias: 0.97542560\n",
      "epoch: 18  loss: 20.14686966  weight: 1.96794486  bias: 0.97448879\n",
      "epoch: 19  loss: 20.13283157  weight: 1.97784472  bias: 0.97425461\n",
      "epoch: 20  loss: 20.12544441  weight: 1.97075832  bias: 0.97351605\n",
      "epoch: 21  loss: 20.12150192  weight: 1.97587693  bias: 0.97314036\n",
      "epoch: 22  loss: 20.11933136  weight: 1.97222614  bias: 0.97250438\n",
      "epoch: 23  loss: 20.11807442  weight: 1.97487617  bias: 0.97205585\n",
      "epoch: 24  loss: 20.11729431  weight: 1.97299898  bias: 0.97147304\n",
      "epoch: 25  loss: 20.11675072  weight: 1.97437465  bias: 0.97098714\n",
      "epoch: 26  loss: 20.11634254  weight: 1.97341311  bias: 0.97043204\n",
      "epoch: 27  loss: 20.11599731  weight: 1.97413087  bias: 0.96992713\n",
      "epoch: 28  loss: 20.11568642  weight: 1.97364187  bias: 0.96938658\n",
      "epoch: 29  loss: 20.11539078  weight: 1.97402012  bias: 0.96887207\n",
      "epoch: 30  loss: 20.11510277  weight: 1.97377515  bias: 0.96833932\n",
      "epoch: 31  loss: 20.11482239  weight: 1.97397792  bias: 0.96782011\n",
      "epoch: 32  loss: 20.11454391  weight: 1.97385907  bias: 0.96729159\n",
      "epoch: 33  loss: 20.11427307  weight: 1.97397113  bias: 0.96677017\n",
      "epoch: 34  loss: 20.11399078  weight: 1.97391748  bias: 0.96624410\n",
      "epoch: 35  loss: 20.11371613  weight: 1.97398257  bias: 0.96572185\n",
      "epoch: 36  loss: 20.11344719  weight: 1.97396278  bias: 0.96519727\n",
      "epoch: 37  loss: 20.11317062  weight: 1.97400367  bias: 0.96467477\n",
      "epoch: 38  loss: 20.11289406  weight: 1.97400093  bias: 0.96415120\n",
      "epoch: 39  loss: 20.11262512  weight: 1.97402966  bias: 0.96362883\n",
      "epoch: 40  loss: 20.11234856  weight: 1.97403550  bias: 0.96310604\n",
      "epoch: 41  loss: 20.11207962  weight: 1.97405815  bias: 0.96258402\n",
      "epoch: 42  loss: 20.11181068  weight: 1.97406840  bias: 0.96206188\n",
      "epoch: 43  loss: 20.11152840  weight: 1.97408783  bias: 0.96154028\n",
      "epoch: 44  loss: 20.11125946  weight: 1.97410047  bias: 0.96101874\n",
      "epoch: 45  loss: 20.11099052  weight: 1.97411799  bias: 0.96049756\n",
      "epoch: 46  loss: 20.11071587  weight: 1.97413206  bias: 0.95997655\n",
      "epoch: 47  loss: 20.11044693  weight: 1.97414851  bias: 0.95945585\n",
      "epoch: 48  loss: 20.11017418  weight: 1.97416317  bias: 0.95893538\n",
      "epoch: 49  loss: 20.10990334  weight: 1.97417915  bias: 0.95841521\n"
     ]
    }
   ],
   "source": [
    "#num of epochs\n",
    "epochs = 50\n",
    "#to store losses at each epoch\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    #prediction\n",
    "    y_pred = model.forward(X)\n",
    "    #loss due to wrong prediction\n",
    "    loss = criterion(y_pred, y)\n",
    "    #store the loss\n",
    "    losses.append(loss.item())\n",
    "    #print the current weight and bias\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}  weight: {model.linear.weight.item():10.8f}  \\\n",
    "bias: {model.linear.bias.item():10.8f}') \n",
    "    #reset stored gradient\n",
    "    optimizer.zero_grad()\n",
    "    #back propagation\n",
    "    loss.backward()\n",
    "    #update parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot the loss values over num of epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intial losss are quite high as model started learning losses dropped drastically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model's Prediction**\n",
    "\n",
    "Now we'll derive <tt>y1</tt> from the trained model to plot the most recent best-fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current weight: 1.97419441, Current bias: 0.95789522\n",
      "\n",
      "[ 1. 50.]\n",
      "[ 2.9320896 99.66761  ]\n"
     ]
    }
   ],
   "source": [
    "w1,b1 = model.linear.weight.item(), model.linear.bias.item()\n",
    "print(f'Current weight: {w1:.8f}, Current bias: {b1:.8f}')\n",
    "print()\n",
    "\n",
    "y1 = x1*w1 + b1\n",
    "print(x1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb5915f3f40>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(x1,y1,'r')\n",
    "plt.title('Current Model')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend([\"original distribution\", \"predcited distribution\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model is able to predict the underlying distribution with much more clarity and seem almost similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the trained model**\n",
    "\n",
    "Right now <strong><tt>model</tt></strong> has been trained and validated, and seems to correctly predicting the underlying distributions. Let's save this to disk.<br>\n",
    "The tools we'll use are <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><strong><tt>torch.save()</tt></strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.load'><strong><tt>torch.load()</tt></strong></a><br>\n",
    "\n",
    "There are two basic ways to save a model.<br>\n",
    "\n",
    "1. The first saves/loads the `state_dict` (learned parameters) of the model, but not the model class.\n",
    "\n",
    "The syntax follows:<br>\n",
    "- <tt><strong>Save:</strong>&nbsp;torch.save(model.state_dict(), PATH)<br><br>\n",
    "- <strong>Load:</strong>&nbsp;model = TheModelClass(\\*args, \\*\\*kwargs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.load_state_dict(torch.load(PATH))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n",
    "\n",
    "2. The second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly.<br>\n",
    "<tt><strong>Save:</strong>&nbsp;torch.save(model, PATH)<br><br>\n",
    "<strong>Load:</strong>&nbsp;model = torch.load(PATH))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n",
    "\n",
    "In either method, you must call <tt>model.eval()</tt> to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "\n",
    "For more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model dict\n",
    "torch.save(model.state_dict(), \"SimpleLinearModel.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = SimpleModel(1, 1)\n",
    "new_model.load_state_dict(torch.load('SimpleLinearModel.pt'))\n",
    "new_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.9320896, 99.66761  ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = new_model.forward(torch.tensor([x1[1]]))\n",
    "    loss = criterion(y_val, torch.tensor([y1[1]]))\n",
    "print(f'{loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's All in PyTorch 101.\n",
    "\n",
    "Happy Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
